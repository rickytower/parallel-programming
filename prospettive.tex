\section{Prospettive sulla programmazione parallela}
Uno degli aspetti più importanti della programmazione parallela, è analizzare il problema e capire se può essere parallelizzato. La parallelizzazione del codice porta a dei miglioramenti delle performance solo se il workload (peso computazionale) è non trascurabile.
Seguono alcune definizioni
\begin{itemize}
    \item \textbf{Task: } unità di esecuzione o di lavoro di un programma o di un sottoprogramma. Le istruzioni sono eseguite sequenzialmente ovvero non in parallelo.

    \item Processo/thread: entità astratta che esegue i task assegnati ai processi. I processi comunicano tra di loro e si sincronizzano per eseguire i loro task.
    \item Processore: motore fisico su cui ciascun processo viene eseguito. Il programma viene scritto come un insieme di processi che vengono poi mappati sul processore.
\end{itemize}

\subsection{Step per creare un programma parallelo}
\begin{figure}[th]
	\centering
	\includegraphics[width=0.7\linewidth]{img/4-step-programma-parallelo}
	\caption{i 4 step per creare un programma parallelo.}
	\label{fig:4-step-programma-parallelo}
\end{figure}

Ci sono 4 passi nella creazione di un programma parallelo:
\begin{enumerate}
    \item Decomposizione dell'algoritmo risolutivo in task o sottoproblemi;
    \item Si assegnano le varie task ai processi; 
    \item Orchestrazione dell'accesso ai dati, comunicazione e sincronizzazione  da parte dei processi;
    \item Mapping dei processi ai processori ()un processo non può essere assegnato a più unità di calcolo).
\end{enumerate}

Il livello di parallelismo viene determinato nelle fasi di decomposizione e assegnamento.

\subsubsection{Comprensione del problema/programma}
Indubbiamente, il primo passo nello sviluppo di software parallelo è capire innanzitutto il problema che si desidera risolvere in parallelo. Se si parte da un programma seriale, ciò implica anche la comprensione del codice esistente.

Prima di investire tempo nel tentativo di sviluppare una soluzione parallela per un problema, bisogna determinare se il problema può effettivamente essere parallelizzato.


\begin{enumerate}
    \item Identificare gli \textbf{hotspot} del programma: rappresentano le parti dove viene svolta la maggior parte del lavoro effettivo. La maggior parte dei programmi scientifici e tecnici di solito svolgono la maggior parte del loro lavoro in pochi punti. Gli strumenti di profilazione e analisi delle prestazioni possono aiutare ad individuare gli hotspote .profiling\footnote{I tool di profiling eseguono il codice e, assieme al suo risultato, ritornano anche un report che può contenere: il numero di invocazioni per ogni funzione contenuta nel codice, e il tempo che impiega ciascuna per determinare il risultato che calcola, quali sono le parti del codice più usate, e così via,\dots} e analisi delle performance (tramite appositi tool). Gli hotspot sono generalmente le sezioni in cui si concentra la parallelizzazione e che hanno un utilizzo della CPU alto.
    \item Identificare i \textbf{bottleneck} del programma: sono le aree di codice che sono più lente da eseguire (come ad esempio le sezioni dedicate all'I/O). Una possibile strada che si può seguire per contrastare i bottleneck, consiste nel trasferire la loro esecuzione sulla GPU, in modo da non dover rallentare la CPU. Dunque, si possono vedere due livelli di parallelismo: il primo dato dalla parallelizzazione su GPU del codice, mentre il secondo dato dalla concorrenza di esecuzione di CPU e GPU.
    \item Identificare gli\textbf{ inibitori al parallelismo}: analizzare il problema significa anche identificare gli inibitori del parallelismo, che sono quei fattori che impediscono di parallelizzare il codice. Una classe comune di inibitori è la dipendenza dei dati, come dimostrato dalla sequenza di Fibonacci sotto.
\end{enumerate}

\underline{Esempio di problema parallelizzabile}: Calcolare l'energia potenziale per migliaia di diverse conformazioni molecolari indipendenti. Una volta completato, individua la conformazione molecolare con l'energia potenziale minima. Ogni conformazione molecolare è determinabile in modo indipendente e il calcolo dell'energia potenziale minima è parallelizzabile.
\\

\underline{Esempio di problema non-parallelizzabile}: Calcolare la serie di Fibonacci. Questo problema non è parallelizzabile in quanto il calcolo attuale dipende dai calcoli precedenti (il termine $k+2$ è dato dalla somma dei termini $k+1$ e $k$). 
\\



\subsubsection{Decomposizione}
Nella decomposizione (o scomposizione) di un problema in task, è cruciale individuarne un numero appropriato in modo che ci siano sempre thread sufficienti per mantenere i processori occupati. In una configurazione con 100 processori funzionanti in parallelo, è necessario avere un ampio pool di task per garantire che tutti i processori siano costantemente utilizzati, anche se uno dei compiti si blocca. È comune che un task raggiunga un punto in cui necessita di dati da un altro task o deve eseguire operazioni di I/O, interrompendo temporaneamente l'esecuzione e lasciando il processore inutilizzato. L'obiettivo è minimizzare questa inattività, assicurando che un nuovo task sia pronto per essere eseguito non appena un processore si libera. Ogni core o processore lasciato inattivo rappresenta uno spreco di risorse. Pertanto, è essenziale disporre di un numero sufficiente di task prontamente disponibili per sostituire quelli bloccati e massimizzare l'utilizzo dei processori.

Esistono due modi per scomporre in task:
\begin{itemize}
    \item \textbf{Scomposizione di dominio:} il dominio può essere suddiviso tra le istanze del task in diverse modalità a seconda delle dimensioni del problema (figura \ref{fig:domain-decomposition}). Ad esempio, se si considera la somma di tutti i dati di un array, questa operazione può essere suddivisa a livello di blocchi o a livello ciclico (figura \ref{fig:slicing-techniques}). Nel primo caso, i dati sono divisi in blocchi distinti assegnati a diverse istanze per l'elaborazione. Nel secondo caso, ogni istanza elabora sequenzialmente una cella di memoria dopo l'altra, seguendo un ciclo, e le celle sono divise da una distanza. 
        \begin{figure}[th]
    	\centering
    	\includegraphics[width=0.7\linewidth]{img/domain-decomposition.png}
    	\caption{scomposizione di dominio.}
    	\label{fig:domain-decomposition}
    \end{figure}
    
    Anche nel caso di domini a due dimensioni, come le matrici, è possibile suddividere l'elaborazione in vari modi. Si possono suddividere i dati in blocchi orizzontali, verticali o a mattonelle. Altrimenti, l'elaborazione può essere ciclica nelle righe, nelle colonne o negli elementi della matrice. Un fattore importante da tenere presente è il bilanciamento del carico (\textbf{load balancing}). se siamo sicuri che il tempo impiegato dalla prima thread sul blocco A corrisponde a quello impiegato dalla seconda thread sul blocco B e così via per il resto dei blocchi, allora abbiamo un carico bilanciato. Questo di solito è il caso per problemi lineari. Quando però abbiamo problemi più complessi, il carico di lavoro di una thread diventa imprevedibile. In questo caso potrebbe essere più ottimale aumentare i blocchi facendo in modo che il primo processore che si libera vada a prendere la successiva thread in attesa.
    \begin{figure}[th]
    	\centering
    	\includegraphics[width=0.7\linewidth]{img/slicing-techniques.png}
    	\caption{i vari modi in cui un dominio può essere scomposto.}
    	\label{fig:slicing-techniques}
    \end{figure}

    \item \textbf{Scomposizione funzionale:} ci si concentra sull'elaborazione che deve essere fatta piuttosto che sui dati manipolati dalla computazione. Il problema è quindi scomposto rispetto al lavoro che deve essere fatto. Ogni task esegue parte del lavoro totale (figura \ref{fig:functional-decomposition}).
        \begin{figure}[th]
    	\centering
    	\includegraphics[width=0.7\linewidth]{img/functional-decomposition.png}
    	\caption{scomposizione funzionale.}
    	\label{fig:functional-decomposition}
    \end{figure}
\end{itemize}

La scomposizione è compito del programmatore, che può essere supportato da tool automatici (campo di ricerca).

\subsubsection*{Esempio di parallelizzazione}
Data un'immagine $N\times N$ vogliamo:
\begin{enumerate}[label=$\bullet$ \textbf{Step \arabic*:}]
    \item raddoppiare la luminosità di ogni pixel; {\color{NavyBlue}\textbf{(complessità $N^2$)}}
    \item calcolare la media di tutti i pixel. {\color{NavyBlue}\textbf{(complessità $N^2$)}}
\end{enumerate}

\paragraph{Soluzione sequenziale.} Una soluzione sequenziale di questo problema costa un tempo totale di {\color{NavyBlue}$N^2+N^2=2N^2$} (figura \ref{fig:esecuzione-sequenziale}). 
\begin{figure}[th]
	\centering
	\includegraphics[width=0.7\linewidth]{img/esecuzione-sequenziale.png}
	\caption{Esecuzione sequenziale.}
	\label{fig:esecuzione-sequenziale}
\end{figure}

\paragraph{Parallelizzazione sulla prima fase.}Una strategia di parallelizzazione consiste nell'eseguire in parallelo lo step 1, completandolo in $\frac{N^2}{P}$ (figure \ref{fig:esecuzione-parallela-1}), dove $P$ rappresenta il numero di processori disponibili. Invece parallelizzare lo step 2 risulta inefficiente  per via della interdipendenza tra i pixel dell'immagine nell'applicazione del filtro, dunque il suo costo è di $N^2$. Lo speedup stimato risulta:
\begin{align*}
    speedup &\le \frac{2N^2}{\frac{N^2}{P} + N^2}\\
    &\le 2
\end{align*}

\begin{figure}[th]
	\centering
	\includegraphics[width=0.7\linewidth]{img/esecuzione-parallela-1.png}
	\caption{Primo tentativo di parallelismo.}
	\label{fig:esecuzione-parallela-1}
\end{figure}

\paragraph{Parallelizzazione sulla seconda fase}Un miglioramento potrebbe essere ottenuto parallelizzando il calcolo della media su ${P}$ processori (calcolo su $P$ matrici di dimensione $\frac{ N^2}{P}$ -- figura \ref{fig:esecuzione-parallela-2}). Infine viene introdotto un tempo di overhead dovuto alla combinazione delle somme parallele per ottenere il risultato finale. I tempi diventano:
\begin{align*}
    \text{Tempo step 1 } &= \frac{N^2}{P}\\
    \text{Tempo step 2 } &= \frac{N^2}{P}+P    
\end{align*}

e si ottiene uno speedup pari a:
\begin{align*}
    speedup \le \frac{2N^2}{\frac{2N^2}{P} + P}\\
\end{align*}

\begin{figure}[th]
	\centering
	\includegraphics[width=0.7\linewidth]{img/esecuzione-parallela-2.png}
	\caption{Secondo tentativo di parallelismo.}
	\label{fig:esecuzione-parallela-2}
\end{figure}

\subsubsection{Assegnamento}
Questo processo richiede la creazione di un numero adeguato di task e la loro distribuzione tra i processi disponibili.

Per prima cosa, è necessario determinare quanti processi sono necessari e quindi associare i task ai processi. Tipicamente, il numero di processi è uguale al numero di processori disponibili per garantire una concorrenza massima.

La strategia di assegnazione dipende dall'esperienza e dalla conoscenza del problema. È consigliabile raggruppare task che richiedono comunicazione tra loro nello stesso processo per accelerare la comunicazione. Al contrario, task che possono eseguire indipendentemente possono essere distribuiti tra processi diversi per ridurre il carico su ciascun processore.

Tuttavia, è cruciale considerare il bilanciamento del carico (\textbf{load balancing}) durante l'assegnazione dei task. Un'assegnazione sbilanciata può compromettere le prestazioni del sistema anche se il codice è parallelizzato correttamente. Ad esempio, se alcuni processi ricevono task più pesanti di altri, potrebbero diventare il collo di bottiglia del sistema, rallentando l'intera esecuzione.

Il tempo di esecuzione complessivo del codice è determinato dal tempo impiegato dal processo più lento, non dalla media dei tempi di esecuzione. Di conseguenza, è fondamentale evitare sbilanciamenti nella distribuzione dei task per massimizzare l'utilizzo dei processori disponibili.

Il miglioramento dell'assegnamento dei task può ridurre gli sprechi di risorse e migliorare le prestazioni complessive del sistema. Anche se non è possibile ottenere un equilibrio perfetto, è importante ridurre al minimo gli sbilanciamenti per sfruttare al meglio le risorse disponibili. Questa ottimizzazione richiede una buona capacità di programmazione e una solida comprensione del problema e dell'architettura del sistema.


\begin{figure}[th]
	\centering
	\includegraphics[width=0.7\linewidth]{img/barrier-sync.png}
	\caption{Esempi di punto di sincronizzazione.}
	\label{fig:barrier-sync}
\end{figure}

Il bilanciamento del carico è cruciale per ottenere prestazioni ottimali in un ambiente di elaborazione parallela, come quello di CUDA. Esistono varie strategie per garantire un bilanciamento efficace del carico di lavoro.

In alcuni casi, è possibile determinare staticamente il peso computazionale di ciascun processo attraverso il profiling e l'analisi delle prestazioni. Tuttavia, ci sono situazioni in cui il peso computazionale dipende dall'input o da altri fattori dinamici, rendendo difficile una determinazione statica. Un esempio di ciò è l'algoritmo di attraversamento del grafo BFS (Breadth-First Search), dove il numero di figli di ciascun nodo può variare notevolmente.

Per gestire questa incertezza, è necessario utilizzare tecniche di bilanciamento dinamico del carico. Queste tecniche consentono di assegnare i task ai processi durante l'esecuzione, in base alle condizioni attuali. Sebbene queste tecniche possano aggiungere un carico computazionale aggiuntivo, sono cruciali per garantire un bilanciamento efficace del carico e massimizzare lo speedup complessivo del sistema.

Durante l'esecuzione, i task vengono inseriti in una coda e assegnati ai processi disponibili. Ogni processo prende un task dalla coda e lo elabora, passando al successivo una volta completato il lavoro. Questo approccio permette di gestire dinamicamente il carico di lavoro e adattarsi alle variazioni nelle prestazioni dei task e dei processi.

Per quanto riguarda la granularità dei task, si parla di task \textbf{coarse-grained} e \textbf{fine-grained}. Questa distinzione si basa sulla quantità di lavoro svolto da ciascun task. I task fine-grained sono piccoli e comprendono poche righe di codice, mentre i task coarse-grained sono più grandi e comprendono una quantità maggiore di codice. La scelta tra task coarse-grained e fine-grained dipende dal problema specifico e dalla necessità di bilanciare il carico di lavoro. I task fine-grained sono più facili da bilanciare e consentono un maggior grado di flessibilità nell'assegnazione dei task ai processi, ma possono introdurre un overhead aggiuntivo a causa delle sincronizzazioni. D'altra parte, i task coarse-grained sono più difficili da bilanciare ma possono ridurre l'overhead di sincronizzazione. La scelta dipende quindi dalle esigenze specifiche del problema e dall'architettura del sistema.
\subsubsection{Orchestrazione}
Il focus principale è comprendere come far comunicare e sincronizzare tutti questi task. Si è già trattato dell'architettura \textit{embarassingly paralle}l, in cui i processi sono completamente indipendenti. In questo scenario, la sincronizzazione non è necessaria: è quasi un'utopia. Tuttavia, quando si rende necessaria la comunicazione, si introduce inevitabilmente un overhead rispetto al codice sequenziale.

I vari tipi di sincronizzazione possono essere descritti come segue:

\begin{enumerate}
	\item \textbf{Sincrona}: Si riferisce a una comunicazione che avviene a intervalli regolari e prevedibili. Le parti coinvolte nel processo di comunicazione, sia il mittente che il ricevente, sono consapevoli degli intervalli in cui scambiare informazioni.
	
	\item \textbf{Asincrona}: In questo caso, la comunicazione avviene senza un tempo predefinito. La trasmissione dei dati può essere iniziata quando necessario, senza dover attendere un intervallo specifico.
\end{enumerate}

A seconda del modo in cui avviene la sincronizzazione, può essere:

\begin{itemize}
	\item \textbf{Bloccante}: Qui, il processo mittente attende il completamento della trasmissione prima di procedere ulteriormente. Si blocca fino a quando il processo destinatario non riceve i dati e viceversa. La sincronizzazione avviene solo quando entrambe le parti sono pronte.
	
	\item \textbf{Non bloccante}: In questo caso, il processo mittente può inviare i dati indipendentemente dallo stato del processo destinatario. La sincronizzazione non richiede l'attesa reciproca e il processo mittente può continuare anche se il processo destinatario non è ancora pronto a ricevere i dati.
\end{itemize}

Inoltre, in un ambiente in cui sono presenti diversi processi, la comunicazione può avvenire secondo due modelli principali:

\begin{itemize}
	\item \textbf{Point-to-point}: Due processi comunicano direttamente tra loro. Di solito, si fa riferimento a un mittente e a un ricevente, o a un produttore e a un consumatore, dove il mittente genera i dati e il ricevente li consuma.
	
	\item \textbf{Collettiva}: In questo caso, un processo comunica con tutti gli altri processi simultaneamente, o un processo può richiedere il contributo di molti altri processi, raccogliendo dati da ognuno di essi.
\end{itemize}
\subsubsection*{Esempio di comunicazione collettiva}
\begin{figure}[th]
	\centering
	\includegraphics[width=0.7\linewidth]{img/comunicazione-collettiva}
	\caption{comunicazione collettiva.}
	\label{fig:comunicazione-collettiva}
\end{figure}

Fare riferimento alla figura \ref{fig:comunicazione-collettiva}. Nella comunicazione in broadcast, un processo invia dati in broadcasting a tutti i processi che possono ascoltare, ripetendo lo stesso dato a tutti. Lo scattering è simile, ma i dati inviati a tutti sono diversi. In presenza di una mole di dati, questi vengono divisi e inviati a ciascun processo, che si occupa di filtrare la sua parte. Il contrario è il gathering, dove i dati elaborati vengono raccolti. Dopo lo scattering, viene eseguito il gathering per raccogliere i risultati e metterli insieme. Vi è anche la modalità di reduction, simile al gathering ma con un'operazione aggiuntiva eseguita contemporaneamente. Si raccolgono i dati e si esegue un'operazione contemporaneamente, e il risultato ricevuto è il risultato dell'operazione. Ad esempio, si può eseguire una reduction con la somma, ricevendo come risultato la somma di tutti i contributi.

Parlando dei tipi di sincronizzazione, le barriere sono presenti in tutti i tipi di linguaggi e architetture parallele. A cosa servono? Se vi sono 16 processi che eseguono qualcosa, si deve aspettare che tutti i 16 abbiano finito prima di procedere. Il processo 1, ad esempio, potrebbe aver completato la sua parte, ma deve aspettare che anche gli altri processi terminino, poiché il resto del compito dipende dai risultati degli altri. Ad esempio, se vi è un accesso in memoria in cui tutti i 16 devono prendere una fetta di dati e caricarla in una memoria condivisa, ognuno deve fare la sua parte. Successivamente, ogni processo può eseguire un'elaborazione che richiede tutti i contributi. Prima di procedere, tutti devono terminare, compreso il processo 1. Quando tutti devono finire prima di procedere, viene inserita una barriera nel codice. La barriera è una riga di codice che dipende dal linguaggio. Quando un processo raggiunge questa linea di codice, verifica il valore del contatore della barriera. Se il contatore è già a 16, il processo procede. Altrimenti, il processo si blocca e incrementa il contatore. Tutti i processi che raggiungono la stessa istruzione incrementano il contatore. Quando l'ultimo processo raggiunge la barriera, il contatore raggiunge 16 e la barriera sblocca tutti i processi.

Le barriere richiedono una sincronizzazione dell'accumulatore condiviso attraverso semafori. Si distinguono sincronizzazioni \textbf{point-to-point} e \textbf{collettive}, quest'ultime coinvolgono un singolo produttore e molti consumatori o viceversa. Il mapping dei processi sui processori è generalmente gestito dal sistema operativo, ma può diventare complesso in \textbf{architetture eterogenee}. Il profiling viene utilizzato per valutare le velocità dei processori e implementare algoritmi di mapping dinamici.

I passaggi di decomposizione, mappatura e assegnazione dipendono dall'architettura su cui viene eseguito il codice. Alcuni sono più ad alto livello e indipendenti dall'architettura, mentre altri dipendono strettamente da essa. Questi ultimi sono compiti del programmatore, che impiega più tempo ma ottiene maggiori benefici e speedup.
\begin{figure}[th]
	\centering
	\includegraphics[width=0.45\linewidth]{img/grid}
	\caption{grid solver. L'espresssione per aggiornare ogni punto interno è: $A[i,j]=0.2 \times(A[i,j]+A[i,j-1]+A[i-1,j]+A[i,j+1]+A[i+1,j])$}
	\label{fig:grid}
\end{figure}

Un problema comune è il calcolo della media dei punti in una griglia, considerando i punti limitrofi (figura \ref{fig:grid}). In sequenza, si itera attraverso la griglia e si calcola la media. In parallelo, la decomposizione potrebbe essere naturale, ma il problema sorge quando è necessario avere i dati dei punti limitrofi per aggiornare il valore corrente. È importante considerare che il risultato finale deve essere la media corretta dei punti.

Una soluzione per risolvere il problema è l'approccio \textbf{GAUS-Seidel}, che applica una formula in parallelo due volte e misura la differenza tra i valori aggiornati. Se la differenza è significativa, ripete l'operazione. Si basa sul concetto di convergenza, dove la differenza tra i valori vecchi e nuovi diminuisce gradualmente fino a quando i valori convergono. Una volta raggiunta la convergenza, il processo termina.

\subsubsection{Implementazione dell'approccio GAUS-Seidel}

Per implementare l'approccio GAUS-Seidel, si può utilizzare una versione sequenziale della formula applicata due volte, con un controllo sulla differenza tra i valori aggiornati per determinare la convergenza. Questo approccio sfrutta il concetto di convergenza, dove la differenza tra i valori vecchi e nuovi diminuisce gradualmente fino alla convergenza, indicando il completamento del processo.
\begin{figure}[th]
	\centering
	\includegraphics[width=0.45\linewidth]{img/red-black-diagonals}
	\caption{Gauss-Seidel con diagonali: in rosso i passaggi del sequenziale e in blu del parallelo.}
	\label{fig:red-black-diagonals}
\end{figure}

\paragraph{\color{red}Nota!}Le frecce {\color{red}rosse} indicano le dipendenze.
\paragraph{Implementazione dell'approccio GAUS-Seidel: parallelizzazione sulle diagonali}

Un problema significativo riguarda il livello di parallelismo all'interno delle diagonali generate (figura \ref{fig:red-black-diagonals}). Il principale ostacolo risiede nella variazione delle dimensioni delle diagonali, che comporta un numero variabile di nodi da sincronizzare. Questa variazione crea un overhead considerevole nell'esecuzione del codice parallelo, compromettendo i vantaggi ottenuti dal parallelismo. Inoltre, il numero massimo di nodi in parallelo corrisponde alla dimensione massima delle diagonali. Ad esempio, in CUDA si possono avere fino a 600 thread, limitando ulteriormente il potenziale di parallelismo. Di conseguenza, sebbene il massimo livello di parallelismo sia raggiunto nelle diagonali quasi complete, in altri casi, come quelli mostrati, il parallelismo è notevolmente ridotto, con solo due task in esecuzione e un'elevata quantità di overhead. Questa situazione compromette l'efficacia complessiva del sistema. Pertanto, come affrontare questa sfida senza compromettere l'efficienza del parallelismo è una questione fondamentale.

\subsubsection{Applicazione dell'approccio Red-Black Ordering}
\begin{figure}[th]
	\centering
	\includegraphics[width=0.5\linewidth]{img/red-black-ordering}
	\caption{Red-Black Ordering. La versione completamente parallelizzabile.}
	\label{fig:red-black-ordering}
\end{figure}
L'approccio Red-Black Ordering suddivide i nodi in due classi, rossi e neri, e applica l'algoritmo in due fasi separate. Prima vengono elaborati tutti i nodi rossi, indipendenti tra loro, e successivamente quelli neri. Questo consente un elevato livello di parallelismo, poiché i nodi di ogni colore possono essere elaborati in parallelo. Nonostante richieda qualche passaggio aggiuntivo, l'approccio convergerà comunque, offrendo un notevole aumento del parallelismo rispetto all'approccio GAUS-Seidel. 

\paragraph{\color{red}Nota!}Le frecce indicano le dipendenze.

\section{Implementazione della comunicazione e della sincronizzazione}

L'implementazione della comunicazione e della sincronizzazione avviene attraverso diverse tecniche, tra cui l'uso di primitive hardware come le lock, le barriere o i semafori. In contesti come OpenMP, sono disponibili specifiche primitive per la gestione di thread. Nel caso di memoria condivisa, l'implementazione delle operazioni di sincronizzazione può essere personalizzata.

È fondamentale che il codice sia comprensibile da tutte le thread create nel sistema, anche se ciascuna thread può interpretarlo in modo diverso. Le thread eseguono il medesimo codice, ma ciascuna identifica il proprio ruolo in base all'istanza di esecuzione.

Si possono sfruttare due tipi di parallelismo: sulle funzioni, dove ogni thread esegue una funzione specifica, o sui dati, dove ciascuna thread opera su dati distinti. È possibile identificare ciascuna thread attraverso funzioni come \texttt{getThreadId()}, consentendo di assegnare compiti specifici in base all'identificazione.

Le tecniche di sincronizzazione, come le barriere, sono disponibili nei linguaggi di programmazione, ma è consigliabile limitarne l'uso a causa del loro impatto sulle prestazioni. In alcune situazioni, è necessario mantenere tali sincronizzazioni per garantire la correttezza dell'esecuzione.
\begin{figure}[th]
	\centering
	\includegraphics[width=0.7\linewidth]{img/message-passing}
	\caption{message passing.}
	\label{fig:message-passing}
\end{figure}

In scenari con memoria distribuita, la comunicazione avviene attraverso primitive di \textbf{message passing}. I processi devono invocare funzioni di invio e ricezione per scambiare dati tra di loro. Per ridurre l'overhead, è consigliabile raggruppare i dati in pacchetti da inviare e ricevere in una singola operazione.

Una strategia comune per gestire la comunicazione su architetture con memoria distribuita è l'uso di buffer intermedi dove i dati sono temporaneamente memorizzati prima della comunicazione. Questa tecnica riduce l'overhead associato alla trasmissione dei dati e facilita l'esecuzione delle computazioni distribuite.

Nonostante il codice rimanga simile, l'implementazione delle operazioni di invio e ricezione varia a seconda del contesto. È importante adattare queste implementazioni in base alle specifiche esigenze dell'architettura e delle applicazioni.
